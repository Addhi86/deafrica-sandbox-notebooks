{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy Assessment of WOfS Product in Africa using Ground Truth Data  <img align=\"right\" src=\"../Supplementary_data/DE_Africa_Logo_Stacked_RGB_small.jpg\">\n",
    "\n",
    "* **Products used:** \n",
    "[ga_ls8c_wofs_2](https://explorer.digitalearth.africa/ga_ls8c_wofs_2),\n",
    "[ga_ls8c_wofs_2_summary ](https://explorer.digitalearth.africa/ga_ls8c_wofs_2_summary),\n",
    "[usgs_ls8c_level2_2]()\n",
    "\n",
    "Notes:\n",
    "* Landsat 8 collection 2 is confidential at continental level on 26 June 2020.\n",
    "* This notebook should be run in Collection 2 Read Private Workspace should we need to run the Landsat 8 Collection 2 Sample dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "The [Water Observations from Space (WOfS)](https://www.ga.gov.au/scientific-topics/community-safety/flood/wofs/about-wofs) is a derived product from Landsat 8 satellite observations as part of provisional Landsat 8 Collection 2 surface reflectance and shows surface water detected in Africa.\n",
    "Individual water classified images are called Water Observation Feature Layers (WOFLs), and are created in a 1-to-1 relationship with the input satellite data. \n",
    "Hence there is one WOFL for each satellite dataset processed for the occurrence of water.\n",
    "\n",
    "The data in a WOFL is stored as a bit field. This is a binary number, where each digit of the number is independantly set or not based on the presence (1) or absence (0) of a particular attribute (water, cloud, cloud shadow etc). In this way, the single decimal value associated to each pixel can provide information on a variety of features of that pixel. \n",
    "For more information on the structure of WOFLs and how to interact with them, see [Water Observations from Space](../Datasets/Water_Observations_from_Space.ipynb) and [Applying WOfS bitmasking](../Frequently_used_code/Applying_WOfS_bitmasking.ipynb) notebooks. \n",
    "\n",
    "Accuracy assessment for WOfS product in Africa includes generating a confusion error matrix for a WOFL binary classification.\n",
    "The inputs for the estimating the accuracy of WOfS derived product are a binary classification WOFL layer showing water/non-water and a shapefile containing validation points collected by [Collect Earth Online](https://collect.earth/) tool. Validation points are the ground truth or actual data while the extracted value for each location from WOFL is the predicted value. A confusion error matrix containing overall, producer's and user's accuracy is the output of this analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "This notebook explains how you can perform accuracy assessment for WOFS derived product using collected ground truth dataset. \n",
    "\n",
    "The notebook demonstrates how to:\n",
    "\n",
    "1. Load validation points for each partner institutions as a list of observations each has a location and month\n",
    "2. Query WOFL data for validation points and capture available WOfS observation available\n",
    "3. Extract statistics for each WOfS observation in each validation point including min, max and mean values for each point (location and month)\n",
    "4. Extract a LUT for each point that contains both validation info and WOfS result for each month \n",
    "5. Generating a confusion error matrix for WOFL classification\n",
    "6. Assessing the accuracy of the classification \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Two extreme cases: \n",
    "    - only test wofs classifier and excluding clouds is ok \n",
    "     - keep clear observations and remove non-clear ones\n",
    "     - then query on those that are water/non-water\n",
    "    - include terrain so water observed and no terrain is predicted true "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "\n",
    "To run this analysis, run all the cells in the notebook, starting with the \"Load packages\" cell.\n",
    "\n",
    "After finishing the analysis, you can modify some values in the \"Analysis parameters\" cell and re-run the analysis to load WOFLs for a different location or time period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load packages\n",
    "Import Python packages that are used for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import time \n",
    "import datacube\n",
    "from datacube.utils import masking, geometry \n",
    "import sys\n",
    "import os\n",
    "import dask \n",
    "import rasterio, rasterio.features\n",
    "import xarray\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import geopandas as gpd\n",
    "import subprocess as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy, scipy.ndimage\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") #this will suppress the warnings for multiple UTM zones in your AOI \n",
    "\n",
    "sys.path.append(\"../Scripts\")\n",
    "from rasterio.mask import mask\n",
    "from geopandas import GeoSeries, GeoDataFrame\n",
    "from shapely.geometry import Point\n",
    "from deafrica_plotting import map_shapefile,display_map, rgb\n",
    "from deafrica_spatialtools import xr_rasterize\n",
    "from deafrica_datahandling import wofs_fuser, mostcommon_crs,load_ard,deepcopy\n",
    "from deafrica_dask import create_local_dask_cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to the datacube\n",
    "Activate the datacube database, which provides functionality for loading and displaying stored Earth observation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = datacube.Datacube(app='WOfS_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up a Dask cluster\n",
    "\n",
    "Dask can be used to better manage memory use and conduct the analysis in parallel. \n",
    "For an introduction to using Dask with Digital Earth Africa, see the [Dask notebook](../Beginners_guide/08_Parallel_processing_with_dask.ipynb).\n",
    "\n",
    ">**Note**: We recommend opening the Dask processing window to view the different computations that are being executed; to do this, see the *Dask dashboard in DE Africa* section of the [Dask notebook](../Beginners_guide/08_Parallel_processing_with_dask.ipynb).\n",
    "\n",
    "To activate Dask, set up the local computing cluster using the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:36183</li>\n",
       "  <li><b>Dashboard: </b><a href='/user/neginm/proxy/8787/status' target='_blank'>/user/neginm/proxy/8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>1</li>\n",
       "  <li><b>Cores: </b>2</li>\n",
       "  <li><b>Memory: </b>14.18 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:36183' processes=1 threads=2, memory=14.18 GB>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "create_local_dask_cluster()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load validation points for each partner institutions as a list of observations each has a location and month\n",
    "    * Load the`csv` validation file as pandas dataframe\n",
    "    * Convert the pandas dataframe into ground_truth `shapefile`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the pandas geo-dataframe to a shapefile and save it \n",
    "#Read in the validation data csv\n",
    "# CEO = '../Supplementary_data/Validation/Refined/CEO_RCMRD_2020-07-30.csv'\n",
    "# df = pd.read_csv(CEO,delimiter=\",\")\n",
    "# geometry = [Point(xy) for xy in zip(df.LON, df.LAT)]\n",
    "# crs = {'init': 'epsg:4326'} \n",
    "# geo_df = GeoDataFrame(df, crs=crs, geometry=geometry)\n",
    "# geo_df.to_file(driver='ESRI Shapefile', filename='../Supplementary_data/Validation/Refined/groundtruth_RCMRD.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Path to the validation data points shapefile \n",
    "# GT = gpd.read_file('../Supplementary_data/Validation/Refined/groundtruth_RCMRD.shp')\n",
    "# #reproject the shapefile from EPSG:4326 to match WOfS dataset \n",
    "# ground_truth  = GT.to_crs({'init': 'epsg:6933'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = '../Supplementary_data/Validation/subset_clip.shp'\n",
    "#path = '../Supplementary_data/Validation/Refined/groundtruth_RCMRD.shp'\n",
    "path = '../Supplementary_data/Validation/Refined/Check/checkcode_point.shp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open shapefile and ensure its in WGS84 coordinates\n",
    "input_data = gpd.read_file(path).to_crs('epsg:4326')\n",
    "#check the shapfile by plotting it\n",
    "#map_shapefile(input_data, attribute='CLASS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed__0', 'Unnamed__1', 'PLOT_ID', 'LON', 'LAT', 'FLAGGED',\n",
       "       'ANALYSES', 'SENTINEL2Y', 'WATER', 'NO_WATER', 'BAD_IMAGE', 'NOT_SURE',\n",
       "       'CLASS', 'COMMENT', 'MONTH', 'WATERFLAG', 'geometry'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do the accuracy assesssment of the validation in each AEZ , we need to obtain WOfS surface water observation data that corresponds with the labelled input data locations. \n",
    "\n",
    "The function `collect_training_data` takes our shapefile containing class labels and extracts training data from the datacube over the location specified by the input geometries. The function will also pre-process our training data by stacking the arrays into a useful format and removing an `NaN` (not-a-number) values.\n",
    "\n",
    "\n",
    "> **The following cell can take several minutes to run.** The class labels will be contained in the first column of the output array.  If you set `ncpus > 1`, then this function will be run in parallel across the specified number of processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean the first two columns\n",
    "input_data= input_data.drop(['Unnamed__0','Unnamed__1'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = [(x,y) for x, y in zip(input_data.geometry.x, input_data.geometry.y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2018-01-31', '2018-02-28', '2018-03-31', '2018-04-30',\n",
       "               '2018-05-31', '2018-06-30', '2018-07-31', '2018-08-31',\n",
       "               '2018-09-30', '2018-10-31', '2018-11-30', '2018-12-31'],\n",
       "              dtype='datetime64[ns]', freq='M')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.date_range('01-2018','01-2019', freq='M')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample WOfS at the ground truth coordinates\n",
    "To load WOFL data, we can first create a re-usable query as below that will define the time period we are interested in, as well as other important parameters that are used to correctly load the data. \n",
    "\n",
    "As WOFLs are created scene-by-scene, and some scenes overlap, it's important when loading data to `group_by` solar day, and ensure that the data between scenes is combined correctly by using the WOfS `fuse_func`.\n",
    "This will merge observations taken on the same day, and ensure that important data isn't lost when overlapping datasets are combined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate query object \n",
    "#need to rethink the items x and y for the coordinates \n",
    "# query ={'resolution':(-30, 30),\n",
    "#         'align':(15,15),\n",
    "#         'group_by':'solar_day'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can convert the WOFL bit field into a binary array containing True and False values. This allows us to use the WOFL data as a mask that can be applied to other datasets.\n",
    "The `make_mask` function allows us to create a mask using the flag labels (e.g. \"wet\" or \"dry\") rather than the binary numbers we used above. For more details on how to do masking on WOfS, see the [Applying_WOfS_bit_masking](../Frequently_used_code/Applying_WOfS_bitmasking.ipynb) notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat = -1.835721214\n",
    "lon = 30.80427593\n",
    "\n",
    "wofls = dc.load(product =\"ga_ls8c_wofs_2\", lat = (lat-0.0003,lat+0.0003), lon=(lon-0.0003,lon+0.0003), output_crs = 'EPSG:6933', resolution=(-30,30),time='2018-12',align=(15,15),resampling='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a mask for no clear observations \n",
    "#no_clear = {\"cloud_shadow\":True, \"cloud\":True, \"nodata\":True}\n",
    "# Define a mask for wet and clear pixels \n",
    "wet_nocloud = {\"water_observed\":True, \"cloud_shadow\":False, \"cloud\":False,\"nodata\":False}\n",
    "# Define a mask for dry and clear pixels \n",
    "dry_nocloud = {\"water_observed\":False, \"cloud_shadow\":False, \"cloud\":False, \"nodata\":False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1\n"
     ]
    }
   ],
   "source": [
    "wofl_wetnocloud = masking.make_mask(wofls, **wet_nocloud).astype(int) \n",
    "wofl_drynocloud = masking.make_mask(wofls, **dry_nocloud).astype(int)\n",
    "clear = (wofl_wetnocloud | wofl_drynocloud).water.all(dim=['x','y']).values\n",
    "n_clear = clear.sum() #record this and use it to filter out month with no valid data \n",
    "if n_clear > 0:\n",
    "    wet = wofl_wetnocloud.isel(time=clear).water.max().values  #record this as WOfS has seen water in the 3*3 window\n",
    "else:\n",
    "    wet = 0 \n",
    "print(n_clear,wet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geometry({'type': 'Point', 'coordinates': (30.80427593, -1.835721214)}, CRS('epsg:4326'))\n",
      "Geometry({'type': 'Point', 'coordinates': (30.80427593, -1.835721214)}, CRS('epsg:4326'))\n",
      "Geometry({'type': 'Point', 'coordinates': (30.80427593, -1.835721214)}, CRS('epsg:4326'))\n",
      "Geometry({'type': 'Point', 'coordinates': (30.80427593, -1.835721214)}, CRS('epsg:4326'))\n",
      "Geometry({'type': 'Point', 'coordinates': (30.80427593, -1.835721214)}, CRS('epsg:4326'))\n",
      "Geometry({'type': 'Point', 'coordinates': (30.80427593, -1.835721214)}, CRS('epsg:4326'))\n",
      "Geometry({'type': 'Point', 'coordinates': (30.80427593, -1.835721214)}, CRS('epsg:4326'))\n",
      "Geometry({'type': 'Point', 'coordinates': (30.80427593, -1.835721214)}, CRS('epsg:4326'))\n",
      "Geometry({'type': 'Point', 'coordinates': (30.80427593, -1.835721214)}, CRS('epsg:4326'))\n",
      "Geometry({'type': 'Point', 'coordinates': (30.80427593, -1.835721214)}, CRS('epsg:4326'))\n",
      "Geometry({'type': 'Point', 'coordinates': (30.80427593, -1.835721214)}, CRS('epsg:4326'))\n",
      "Geometry({'type': 'Point', 'coordinates': (30.80427593, -1.835721214)}, CRS('epsg:4326'))\n"
     ]
    }
   ],
   "source": [
    "#%time  #in order to get the timing \n",
    "\n",
    "##input_data['CLASS_WET','CLASS_DRY'] = None\n",
    "\n",
    "##Step 1: update the query for WOfS based on CEO input table \n",
    "for index, row in input_data.iterrows():\n",
    "    #i = 0\n",
    "    #print(\" Feature {:04}/{:04}\\r\".format(i + 1, len(input_data)),end='')  #for monitoring performance  \n",
    "    #get the month value for each index\n",
    "    month = input_data.loc[index]['MONTH'] \n",
    "    #set the time for query of the WOfS database according to the month value in the validation table \n",
    "    time = '2018-' + f'{month:02d}' \n",
    "    #this is for having the original query as it is \n",
    "    dc_query = deepcopy(query) \n",
    "    geom = geometry.Geometry(input_data.geometry.values[index].__geo_interface__, geometry.CRS('epsg:4326')) #convert to 6933 before that query and add a small buffer 40m(0.0005)\n",
    "    q = {\"geopolygon\":geom}\n",
    "    t = {\"time\":time} \n",
    "    dc_query.update(t)\n",
    "    dc_query.update(q)\n",
    "    wofls = dc.load(product =\"ga_ls8c_wofs_2\", output_crs = 'EPSG:6933', fuse_func=wofs_fuser,**dc_query).squeeze() \n",
    "    \n",
    "##Step 2: update the table with count of clear observations that are either wet or dry in that particular month \n",
    "\n",
    "    #mask the wofls for wet and clear pixels \n",
    "    wofl_wetnocloud = masking.make_mask(wofls, **wet_nocloud).astype(int) #using astype(int) will convert the original bool value to integer \n",
    "    #testing part\n",
    "    #print(wofl_wetnocloud.x.values,wofl_wetnocloud.y.values)\n",
    "    #testing part\n",
    "    wofl_wet = wofl_wetnocloud.water.sum()#(dim='time') #sum will act as count here \n",
    "    #mask the wofls for wet and dry pixels \n",
    "    wofl_drynocloud = masking.make_mask(wofls, **dry_nocloud).astype(int)\n",
    "    #testing part\n",
    "    #print(wofl_drynocloud.x.values,wofl_drynocloud.y.values)\n",
    "    #testing part\n",
    "    wofl_dry = wofl_drynocloud.water.sum()#(dim='time')\n",
    "    \n",
    "    #testing part \n",
    "    #print(wofl_wet.x.values,wofl_wet.y.values)\n",
    "    #print(wofl_dry.x.values,wofl_dry.y.values)\n",
    "    #testing part \n",
    "    \n",
    "#     #mask the wofls for no clear pixels \n",
    "#     noclear = masking.make_mask(wofls, **no_clear).astype(int)\n",
    "#     wofl_no_clear = noclear.water.sum()\n",
    "    #update the column for the clear and water observation \n",
    "    input_data.at[index,'CLASS_WET'] = wofl_wet.values \n",
    "    #update the column for the clear and no water observaion \n",
    "    input_data.at[index,'CLASS_DRY'] = wofl_dry.values\n",
    "    #update the column for the clear water observation frequency \n",
    "    wofl_clear_observations = wofl_dry + wofl_wet \n",
    "    #update the column for the clear observation \n",
    "    input_data.at[index,'CLEAR_OBS'] = (wofl_clear_observations).values\n",
    "    #update the column for the frequency of clear and wet observations \n",
    "    input_data.at[index,'FREQUENCY'] = (wofl_wet / wofl_clear_observations).values \n",
    "#     if index > 10: \n",
    "#         break \n",
    "#    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_data.to_csv(('../Supplementary_data/Validation/Refined/ground_truth_RCMRD.csv'))\n",
    "input_data.to_csv(('../Supplementary_data/Validation/Refined/Check/checkcode_Wofs.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for checking the outcome of the second filtering with a need to run the entire processing (in above cell)\n",
    "CEO = '../Supplementary_data/Validation/Refined/ground_truth_RCMRD.csv'\n",
    "input_data= pd.read_csv(CEO,delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexNames = (input_data['WATERFLAG'] > 2) & (input_data['CLEAR_OBS'] > 0) & (input_data['FREQUENCY'] > 0) & (input_data['FREQUENCY'] < 1) #that is for or you need to use\n",
    "indexNames.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#as water flag more than 1 and also clear observation equal to zero \n",
    "indexNames = input_data[(input_data['WATERFLAG'] > 1) | (input_data['CLEAR_OBS'] == 0)].index #that is for or you need to use |\n",
    "input_data.drop(indexNames, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this will count on the number of month for each plotID in the final table \n",
    "count = input_data.groupby(['PLOT_ID'])['MONTH'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the check on the count for each plot id as a csv file \n",
    "count.to_csv('../Supplementary_data/Validation/Refined/final_RCMRD_count.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data.to_csv(('../Supplementary_data/Validation/Refined/ground_truth_RCMRD_final.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.2.dev7+gdcab0e02\n"
     ]
    }
   ],
   "source": [
    "print(datacube.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Additional information\n",
    "\n",
    "**License:** The code in this notebook is licensed under the [Apache License, Version 2.0](https://www.apache.org/licenses/LICENSE-2.0). \n",
    "Digital Earth Africa data is licensed under the [Creative Commons by Attribution 4.0](https://creativecommons.org/licenses/by/4.0/) license.\n",
    "\n",
    "**Contact:** If you need assistance, please post a question on the [Open Data Cube Slack channel](http://slack.opendatacube.org/) or on the [GIS Stack Exchange](https://gis.stackexchange.com/questions/ask?tags=open-data-cube) using the `open-data-cube` tag (you can view previously asked questions [here](https://gis.stackexchange.com/questions/tagged/open-data-cube)).\n",
    "If you would like to report an issue with this notebook, you can file one on [Github](https://github.com/digitalearthafrica/deafrica-sandbox-notebooks).\n",
    "\n",
    "**Last modified:** January 2020\n",
    "\n",
    "**Compatible datacube version:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tags\n",
    "Browse all available tags on the DE Africa User Guide's [Tags Index](https://) (placeholder as this does not exist yet)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "**Tags**:  :index:`WOfS`, :index:`fractional cover`, :index:`deafrica_plotting`, :index:`deafrica_datahandling`, :index:`display_map`, :index:`wofs_fuser`, :index:`WOFL`, :index:`masking`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the groundtruth with a 6933 EPSG as well (conversion) - how to reproject"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "9e3fa49adf8c4170abfcd954c2ec045a": {
      "model_module": "jupyter-leaflet",
      "model_module_version": "^0.11.1",
      "model_name": "LeafletZoomControlModel",
      "state": {
       "_model_module_version": "^0.11.1",
       "_view_count": null,
       "_view_module_version": "^0.11.1",
       "options": [
        "position",
        "zoom_in_text",
        "zoom_in_title",
        "zoom_out_text",
        "zoom_out_title"
       ]
      }
     },
     "dc642f11c1fb492ca419b0ed6fc4f8c3": {
      "model_module": "jupyter-leaflet",
      "model_module_version": "^0.11.1",
      "model_name": "LeafletAttributionControlModel",
      "state": {
       "_model_module_version": "^0.11.1",
       "_view_count": null,
       "_view_module_version": "^0.11.1",
       "options": [
        "position",
        "prefix"
       ],
       "position": "bottomright",
       "prefix": "Leaflet"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
