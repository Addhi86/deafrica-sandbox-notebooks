{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentinel-2 Validation Data Analysis - Point-based and Parallel  <img align=\"right\" src=\"../Supplementary_data/DE_Africa_Logo_Stacked_RGB_small.jpg\">\n",
    "\n",
    "* **Products used:** \n",
    "[s2_l2a](https://explorer.digitalearth.africa/s2_l2a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "TBA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "This notebook explains how you can perform validation analysis for S2 SCL layer using collected ground truth dataset and window-based sampling. \n",
    "\n",
    "The notebook demonstrates how to:\n",
    "\n",
    "1. Load validation points for each partner institutions following cleaning stage as an ESRI shapefile\n",
    "2. Query Sentinel-2 SCL layer for validation points and capture available Sentinel-2 observation available\n",
    "3. Extract statistics for each S2 observation in each validation point using multiprocessing functionality \n",
    "4. Extract a LUT for each point that contains both validation info and S2 result for each month \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "\n",
    "To run this analysis, run all the cells in the notebook, starting with the \"Load packages\" cell.\n",
    "\n",
    "After finishing the analysis, you can modify some values in the \"Analysis parameters\" cell and re-run the analysis to load WOFLs for a different location or time period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load packages\n",
    "Import Python packages that are used for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import time \n",
    "import datacube\n",
    "from datacube.utils import masking, geometry \n",
    "import sys\n",
    "import os\n",
    "import dask \n",
    "import rasterio, rasterio.features\n",
    "import xarray as xr\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import geopandas as gpd\n",
    "import subprocess as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy, scipy.ndimage\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") #this will suppress the warnings for multiple UTM zones in your AOI \n",
    "\n",
    "sys.path.append(\"../Scripts\")\n",
    "from rasterio.mask import mask\n",
    "from geopandas import GeoSeries, GeoDataFrame\n",
    "from shapely.geometry import Point\n",
    "from deafrica_plotting import map_shapefile,display_map, rgb\n",
    "from deafrica_spatialtools import xr_rasterize\n",
    "from deafrica_datahandling import wofs_fuser, mostcommon_crs,load_ard,deepcopy\n",
    "from deafrica_dask import create_local_dask_cluster\n",
    "\n",
    "#for parallelisation \n",
    "from multiprocessing import Pool, Manager\n",
    "import multiprocessing as mp\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To analyse validation points collected by each partner institution, we need to obtain WOfS surface water observation data that corresponds with the labelled input data locations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load validation points for each partner institutions as a list of observations each has a location and month\n",
    "    * Load the cleaned validation file as ESRI `shapefile`\n",
    "    * Inspect the shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate query object \n",
    "query ={'resolution':(-20, 20),\n",
    "        'group_by':'solar_day',\n",
    "        'output_crs':'EPSG:6933'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../Supplementary_data/Validation/Refined/groundtruth_RCMRD.shp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed_ 0', 'Unnamed__1', 'PLOT_ID', 'LON', 'LAT', 'FLAGGED',\n",
       "       'ANALYSES', 'SENTINEL2Y', 'WATER', 'NO_WATER', 'BAD_IMAGE', 'NOT_SURE',\n",
       "       'CLASS', 'COMMENT', 'MONTH', 'WATERFLAG', 'geometry'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data = gpd.read_file(path).to_crs('epsg:6933') #reading the table and converting CRS to metric \n",
    "input_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data= input_data.drop(['Unnamed_ 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = [(x,y) for x, y in zip(input_data.geometry.x, input_data.geometry.y)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample S2 at the ground truth coordinates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to sample WOfS for each validation point for early five days of each month \n",
    "def get_S2_for_point(index, row, input_data, query, results_wet, results_clear):\n",
    "    dc = datacube.Datacube(app='S2_accuracy')\n",
    "    #get the month value for each index\n",
    "    month = input_data.loc[index]['MONTH'] \n",
    "    #set the time for query of WOfS database according to the first five days before and after of each calendar month\n",
    "    time = '2018-'+f'{month:02d}'    \n",
    "    plot_id = input_data.loc[index]['PLOT_ID']\n",
    "    #having the original query as it is \n",
    "    dc_query = deepcopy(query) \n",
    "    geom = geometry.Geometry(input_data.geometry.values[index].__geo_interface__, \n",
    "                             geometry.CRS('EPSG:6933'))\n",
    "    q = {\"geopolygon\":geom}\n",
    "    t = {\"time\":time} \n",
    "    \n",
    "    #updating the query\n",
    "    dc_query.update(t)\n",
    "    dc_query.update(q)\n",
    "    \n",
    "    ds = dc.load(product =\"s2_l2a\",\n",
    "                 measurements=['SCL'],\n",
    "                 **dc_query)\n",
    "    \n",
    "    #Check if water is observed by SCL \n",
    "    if np.any(ds.SCL.values==6) == True:\n",
    "        results_wet.update({str(int(plot_id))+\"_\"+str(month) : 1})\n",
    "    else:\n",
    "        results_wet.update({str(int(plot_id))+\"_\"+str(month) : 0})\n",
    "    \n",
    "    #number of clear \n",
    "    n_clear = np.count_nonzero(ds.SCL.isin([2,4,5,6,7]).values)\n",
    "    results_clear.update({str(int(plot_id))+\"_\"+str(month) : int(n_clear)})\n",
    "                          \n",
    "#     wet_nocloud = (ds.SCL == 6).astype(int)\n",
    "#     ds.SCL.values.\n",
    "#     print(xr.ufuncs.isnan(ds.where(wet_nocloud)))\n",
    "    \n",
    "#     S2_wet = wet_nocloud.sum() #this returns one value for a month \n",
    "#     print(S2_wet)\n",
    "#     # Define a mask for dry and clear pixels \n",
    "#     dry_nocloud  = (ds.SCL.isin([2,4,5,7])).astype(int)\n",
    "#     S2_dry = dry_nocloud.sum() #this returns one value for a month \n",
    "#     #Define a mask for dry and clear pixels \n",
    "#     clear = (S2_wet | S2_dry)#.all().values  #My major doubt is in here \n",
    "#     n_clear = clear.sum() #record this and use it to filter out month with no valid data \n",
    "#     if n_clear > 0:\n",
    "#         wet = S2_wet.isel(time=clear).max().values  #record this as S2 has seen water on the day \n",
    "#     else:\n",
    "#         wet = 0 \n",
    "\n",
    "#     results_wet.update({str(int(plot_id))+\"_\"+str(month) : int(wet)})\n",
    "#     results_clear.update({str(int(plot_id))+\"_\"+str(month) : int(n_clear)})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing For-Loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'137387037_1': 0, '137387037_2': 0, '137387037_3': 0, '137387037_4': 1, '137387037_5': 1, '137387037_6': 1, '137387037_7': 1, '137387037_8': 0, '137387037_9': 0, '137387037_10': 0} {'137387037_1': 5, '137387037_2': 2, '137387037_3': 4, '137387037_4': 1, '137387037_5': 3, '137387037_6': 2, '137387037_7': 1, '137387037_8': 2, '137387037_9': 3, '137387037_10': 3}\n"
     ]
    }
   ],
   "source": [
    "results_wet_test = dict()\n",
    "results_clear_test = dict()\n",
    "\n",
    "for index, row in input_data[0:10].iterrows():\n",
    "    get_S2_for_point(index, row, input_data, query, results_wet_test, results_clear_test)\n",
    "\n",
    "print(results_wet_test, results_clear_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel Processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parallel_fun(input_data, query, ncpus):\n",
    "    \n",
    "    manager = mp.Manager()\n",
    "    results_wet = manager.dict()\n",
    "    results_clear = manager.dict()\n",
    "   \n",
    "    # progress bar\n",
    "    pbar = tqdm(total=len(input_data))\n",
    "        \n",
    "    def update(*a):\n",
    "        pbar.update()\n",
    "\n",
    "    with mp.Pool(ncpus) as pool:\n",
    "        for index, row in input_data.iterrows():\n",
    "            pool.apply_async(get_S2_for_point,\n",
    "                                 [index,\n",
    "                                 row,\n",
    "                                 input_data,\n",
    "                                 query,\n",
    "                                 results_wet,\n",
    "                                 results_clear], callback=update)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        pbar.close()\n",
    "        \n",
    "    return results_wet, results_clear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel for runnning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:15<00:00,  1.58it/s]\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "wet, clear = _parallel_fun(input_data[0:25], query, ncpus=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wetdf = pd.DataFrame.from_dict(wet, orient = 'index')\n",
    "cleardf = pd.DataFrame.from_dict(clear,orient='index')\n",
    "df2 = wetdf.merge(cleardf, left_index=True, right_index=True)\n",
    "df2 = df2.rename(columns={'0_x':'CLASS_WET','0_y':'CLEAR_OBS'})\n",
    "#split the index (which is plotid+month) into seperate columns\n",
    "for index, row in df2.iterrows():\n",
    "    df2.at[index,'PLOT_ID'] = index.split('_')[0] +'.0'\n",
    "    df2.at[index,'MONTH'] = index.split('_')[1]\n",
    "#reset the index\n",
    "df2 = df2.reset_index(drop=True)\n",
    "#convert plot id and month to str to help with matching\n",
    "input_data['PLOT_ID'] = input_data.PLOT_ID.astype(str)\n",
    "input_data['MONTH']= input_data.MONTH.astype(str)\n",
    "# merge both dataframe at locations where plotid and month match\n",
    "final_df = pd.merge(input_data, df2, on=['PLOT_ID','MONTH'], how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As water flag more than 1 and also clear observation equal to zero \n",
    "indexNames = final_df[(final_df['WATERFLAG'] > 1) | (final_df['CLEAR_OBS'] == 0) | (final_df['CLEAR_OBS'].isna()) ].index \n",
    "final_df.drop(indexNames, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv(('../Supplementary_data/Validation/Refined/RCMRD_points.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datacube.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Additional information\n",
    "\n",
    "**License:** The code in this notebook is licensed under the [Apache License, Version 2.0](https://www.apache.org/licenses/LICENSE-2.0). \n",
    "Digital Earth Africa data is licensed under the [Creative Commons by Attribution 4.0](https://creativecommons.org/licenses/by/4.0/) license.\n",
    "\n",
    "**Contact:** If you need assistance, please post a question on the [Open Data Cube Slack channel](http://slack.opendatacube.org/) or on the [GIS Stack Exchange](https://gis.stackexchange.com/questions/ask?tags=open-data-cube) using the `open-data-cube` tag (you can view previously asked questions [here](https://gis.stackexchange.com/questions/tagged/open-data-cube)).\n",
    "If you would like to report an issue with this notebook, you can file one on [Github](https://github.com/digitalearthafrica/deafrica-sandbox-notebooks).\n",
    "\n",
    "**Last modified:** September 2020\n",
    "\n",
    "**Compatible datacube version:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tags\n",
    "Browse all available tags on the DE Africa User Guide's [Tags Index](https://) (placeholder as this does not exist yet)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "**Tags**:  :index:`SCL`, :index:`fractional cover`, :index:`deafrica_plotting`, :index:`deafrica_datahandling`, :index:`display_map`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "9e3fa49adf8c4170abfcd954c2ec045a": {
      "model_module": "jupyter-leaflet",
      "model_module_version": "^0.11.1",
      "model_name": "LeafletZoomControlModel",
      "state": {
       "_model_module_version": "^0.11.1",
       "_view_count": null,
       "_view_module_version": "^0.11.1",
       "options": [
        "position",
        "zoom_in_text",
        "zoom_in_title",
        "zoom_out_text",
        "zoom_out_title"
       ]
      }
     },
     "dc642f11c1fb492ca419b0ed6fc4f8c3": {
      "model_module": "jupyter-leaflet",
      "model_module_version": "^0.11.1",
      "model_name": "LeafletAttributionControlModel",
      "state": {
       "_model_module_version": "^0.11.1",
       "_view_count": null,
       "_view_module_version": "^0.11.1",
       "options": [
        "position",
        "prefix"
       ],
       "position": "bottomright",
       "prefix": "Leaflet"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
