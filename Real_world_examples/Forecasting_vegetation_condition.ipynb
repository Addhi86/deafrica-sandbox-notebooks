{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting vegetation condition \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "This notebook conducts multivariate time-series forecasting using [vector autoregression](https://en.wikipedia.org/wiki/Vector_autoregression). We will use soil moisture, retrieved from the Global Rootzone moisture Analysis and Forecasting System (GRAFS), to make a short-term forecast of vegetation condition (using NDVI). GRAFS is produced by the [ANU Centre for Water and Landscape Dynamics](http://wald.anu.edu.au/). The model estimates the surface (0-5 cm) and root-zone (0-1 m) soil moisture at 10 km spatial resolution globally, using the precipitation measured by the Global Precipitation Measurement (GPM) mission and through assimilation of the soil moisture product from the Soil Moisture Active/Passive (SMAP) mission.\n",
    "\n",
    "This product is regularly updated and made available through National Computational Infrastructureâ€™s open access THREDDS data server.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load packages\n",
    "Import Python packages that are used for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from datacube import Datacube\n",
    "\n",
    "from deafrica_tools.datahandling import load_ard\n",
    "from deafrica_tools.plotting import display_map\n",
    "from deafrica_tools.bandindices import calculate_indices\n",
    "from deafrica_tools.dask import create_local_dask_cluster\n",
    "from deafrica_tools.load_era5 import load_era5\n",
    "\n",
    "from statsmodels.tsa.vector_ar.var_model import VAR\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tools.eval_measures import rmse\n",
    "from statsmodels.tsa.stattools import grangercausalitytests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_local_dask_cluster()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis parameters\n",
    "\n",
    "* `lat`, `lon`: The central latitude and longitude to analyse e.g. 14.283, -16.921\n",
    "* `buffer`: The number of square degrees to load around the central latitude and longitude. For reasonable loading times, set this as 0.1 or lower\n",
    "* `time_range`: The date range to analyse e.g. ('2013', '2020')\n",
    "* `freq`: The frequency we want to resample the time-series to e.g. for monthly time steps use `'1M'`, for fortinightly use `'2W'`\n",
    "* `forecast_length`: The number of time-steps (in units of the resampling frequency) to forecast. The longer the forecast, the less accurate the forecast will be. A forecast of vegetation condition will most likley not be accurate beyone 3-4 months.\n",
    "* `dask_chunks`: How to chunk the datasets to work with dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the analysis region (Lat-Lon box)\n",
    "lat, lon = -1.4908, 34.7683 # grasslands in serengeti\n",
    "buffer = 0.02\n",
    "\n",
    "products = \"ls7_sr\", \"ls8_sr\"\n",
    "\n",
    "# Define the time window\n",
    "time_range = ('2015-01-01', '2020-12-30')\n",
    "\n",
    "#resample frequency\n",
    "freq='1M'\n",
    "\n",
    "#length of forecast (in units of resample frequency)\n",
    "forecast_length = 2\n",
    "\n",
    "#dask chunk sizes\n",
    "dask_chunks={'x':-1, 'y':-1, 'time':-1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display analysis area on an interactive map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lon=(lon - buffer, lon + buffer)\n",
    "lat=(lat - buffer, lat + buffer)\n",
    "\n",
    "display_map(lon,lat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve multiple environmental datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the satellite data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to the datacube\n",
    "dc = Datacube(app='VAR-forecast')\n",
    "\n",
    "# set up a datcube query object\n",
    "query = {'x': lon, 'y': lat,\n",
    "         'time': time_range,\n",
    "         'measurements':['red', 'nir'],\n",
    "         'output_crs' :'EPSG:6933',\n",
    "         'resolution' : (-30, 30),\n",
    "         'resampling' :{\"fmask\": \"nearest\", \"*\": \"bilinear\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the satellite data\n",
    "ds = load_ard(dc=dc, \n",
    "              dask_chunks=dask_chunks,\n",
    "              products=products,\n",
    "              **query)\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate NDVI time-series\n",
    "\n",
    "The satellite data will be computed here so this cell with take a few minutes to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate NDVI\n",
    "ndvi = calculate_indices(ds, 'NDVI', drop=True, collection='c2')\n",
    "\n",
    "#interpolate NDVI to a monthly frequency\n",
    "ndvi = ndvi.interpolate_na(dim='time', method='linear',).resample(time=freq).mean()\n",
    "ndvi=ndvi.mean(['x','y'])\n",
    "ndvi = ndvi.NDVI.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndvi.plot(figsize=(11,5),linestyle='dashed', marker='o')\n",
    "plt.title('NDVI')\n",
    "plt.ylim(0,0.9);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval of surface and root-zone wetness\n",
    "\n",
    "> `Surface wetness` is measured relative to wettest condition recorded for a location.\n",
    "\n",
    "> `Rootzone Soil Water Index` is derived from surface relative wetness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load soil moisture data\n",
    "def load_soil_moisture(lat, lon, time, grid=None, product = 'surface'):\n",
    "    product_baseurl = 'http://dapds00.nci.org.au/thredds/dodsC/ub8/global/GRAFS/'\n",
    "    assert product in ['surface', 'rootzone'], 'product parameter must be surface or root-zone'\n",
    "    # define a grid that covers the entire area of interest\n",
    "    lat_range = np.arange(np.max(np.ceil(np.array(lat)*10.+0.5)/10.-0.05),\n",
    "                          np.min(np.floor(np.array(lat)*10.-0.5)/10.+0.05)-0.05, -0.1)\n",
    "    lon_range = np.arange(np.min(np.floor(np.array(lon)*10.-0.5)/10.+0.05),\n",
    "                          np.max(np.ceil(np.array(lon)*10.+0.5)/10.-0.05)+0.05, 0.1)\n",
    "    # split time window into years\n",
    "    day_range = np.array(time).astype(\"M8[D]\")\n",
    "    year_range = np.array(time).astype(\"M8[Y]\")\n",
    "    if product == 'surface':\n",
    "        product_name = 'GRAFS_TopSoilRelativeWetness_'\n",
    "    else: product_name = 'GRAFS_RootzoneSoilWaterIndex_'\n",
    "    datasets = []\n",
    "    for year in np.arange(year_range[0], year_range[1]+1, np.timedelta64(1, 'Y')):\n",
    "        start = np.max([day_range[0], year.astype(\"M8[D]\")])\n",
    "        end = np.min([day_range[1], (year+1).astype(\"M8[D]\")-1])\n",
    "        product_url = product_baseurl + product_name +'%s.nc'%str(year)\n",
    "        print(product_url)\n",
    "        # data is loaded lazily through OPeNDAP\n",
    "        ds = xr.open_dataset(product_url)\n",
    "        # slice before return\n",
    "        ds = ds.sel(lat=lat_range, lon=lon_range, time=slice(start, end)).compute()\n",
    "        datasets.append(ds)\n",
    "    return xr.merge(datasets)\n",
    "\n",
    "# Retrieve surface soil moisture using query parameters\n",
    "surface_wetness = load_soil_moisture(lat, lon, time_range) #grid='nearest'\n",
    "\n",
    "# retrieve rootzone soil moisture using query parameters\n",
    "rootzone_wetness = load_soil_moisture(lat, lon, time_range, product='rootzone') #grid='nearest'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert all datasets to 1D time-series\n",
    "\n",
    "And resample to common time-step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zonal mean\n",
    "surface_wetness=surface_wetness.relative_wetness.mean(['lat','lon'])\n",
    "rootzone_wetness=rootzone_wetness.soil_water_index.mean(['lat','lon'])\n",
    "\n",
    "#resample using monthly means\n",
    "surface_wetness=surface_wetness.resample(time=freq).mean()\n",
    "rootzone_wetness=rootzone_wetness.resample(time=freq).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot all datasets together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot soil moisture with precipitation\n",
    "fig, ax1 = plt.subplots(figsize=(16,8))\n",
    "ndvi.plot(ax=ax1,label='NDVI',color='red',linestyle='dashed', marker='o')\n",
    "surface_wetness.plot(ax = ax1,label='surface wetness',color='blue',linestyle='dashed', marker='o')\n",
    "rootzone_wetness.plot(ax = ax1,label='root-zone wetness',color='black',linestyle='dashed', marker='o')\n",
    "ax1.set_ylabel('soil wetness / NDVI')\n",
    "ax1.legend(loc='upper left');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert datasets to a pandas dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to dfs\n",
    "ndvi=ndvi.drop('spatial_ref').to_dataframe()\n",
    "surface_wetness=surface_wetness.to_dataframe()\n",
    "rootzone_wetness=rootzone_wetness.to_dataframe()\n",
    "\n",
    "#add other variables to ndvi\n",
    "ndvi['surface_wetness']=surface_wetness['relative_wetness']\n",
    "ndvi['rootzone_wetness']=rootzone_wetness['soil_water_index']\n",
    "\n",
    "#drop an rows with nans\n",
    "ndvi = ndvi.dropna()\n",
    "ndvi.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to standardised anomalies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test causation\n",
    "\n",
    "The basis behind Vector AutoRegression is that each of the time series in the system influences each other. That is, you can predict the series with past values of itself along with other series in the system.\n",
    "Below, we conduct a [Granger's Causality Test](https://en.wikipedia.org/wiki/Granger_causality) to see if the variables are related to each other.  In the table that's printed after running the two cells belows shows a given p-value is < significance level (0.05), then, the corresponding X series (column) causes the Y (row)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grangers_causation_matrix(data, variables, maxlag=12, test='ssr_chi2test', verbose=False):    \n",
    "    \"\"\"Check Granger Causality of all possible combinations of the Time series.\n",
    "    The rows are the response variable, columns are predictors. The values in the table \n",
    "    are the P-Values. P-Values lesser than the significance level (0.05), implies \n",
    "    the Null Hypothesis that the coefficients of the corresponding past values is \n",
    "    zero, that is, the X does not cause Y can be rejected.\n",
    "\n",
    "    data      : pandas dataframe containing the time series variables\n",
    "    variables : list containing names of the time series variables.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(np.zeros((len(variables), len(variables))), columns=variables, index=variables)\n",
    "    for c in df.columns:\n",
    "        for r in df.index:\n",
    "            test_result = grangercausalitytests(data[[r, c]], maxlag=maxlag, verbose=False)\n",
    "            p_values = [round(test_result[i+1][0][test][1],4) for i in range(maxlag)]\n",
    "            if verbose: print(f'Y = {r}, X = {c}, P Values = {p_values}')\n",
    "            min_p_value = np.min(p_values)\n",
    "            df.loc[r, c] = min_p_value\n",
    "    df.columns = [var + '_x' for var in variables]\n",
    "    df.index = [var + '_y' for var in variables]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grangers_causation_matrix(ndvi, variables = ndvi.columns)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test for stationarity \n",
    "\n",
    "VAR models require the time series you want to forecast to be stationary. A stationary time series is one whose characteristics like mean and variance does not change over time. \n",
    "To test the stationarity, we can use a method called the [Augmented Dickey-Fuller Test (ADF Test)](https://www.machinelearningplus.com/time-series/augmented-dickey-fuller-test/).\n",
    "\n",
    "If the time-series are non-stationary, you can make it stationary by differencing the series once and repeating the test again until it becomes stationary. Differencing can be done easily using the following code snippet:\n",
    "\n",
    "    df_differenced = df_train.diff().dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function for conducting an ADF test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adfuller_test(series, signif=0.05, name='', verbose=False):\n",
    "    \"\"\"Perform ADFuller to test for Stationarity of given series and print report\"\"\"\n",
    "    r = adfuller(series, autolag='AIC')\n",
    "    output = {'test_statistic':round(r[0], 4), 'pvalue':round(r[1], 4), 'n_lags':round(r[2], 4), 'n_obs':r[3]}\n",
    "    p_value = output['pvalue'] \n",
    "    def adjust(val, length= 6): return str(val).ljust(length)\n",
    "\n",
    "    # Print Summary\n",
    "    print(f'    ADF Test on \"{name}\"', \"\\n   \", '-'*47)\n",
    "    print(f' Null Hypothesis: Data is non-Stationary.')\n",
    "\n",
    "    if p_value <= signif:\n",
    "        print(f\" => P-Value = {p_value}. Rejecting Null Hypothesis.\")\n",
    "        print(f\" => Series is Stationary.\")\n",
    "    else:\n",
    "        print(f\" => P-Value = {p_value}. Should not reject the Null Hypothesis\")\n",
    "        print(f\" => Series is Non-Stationary.\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADF Test on each column\n",
    "for name, column in ndvi.iteritems():\n",
    "    adfuller_test(column, name=column.name)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conduct iterative back-test to validate forecasting ability\n",
    "\n",
    "Here we will conduct a forecast, but over an interval of the time-series that we already have observations for. This will allow us to test the model's forecasting ability "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, lets initiate a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=VAR(ndvi, freq=freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the best lag order using Akaike information criterion (AIC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate AIC\n",
    "x=model.select_order()\n",
    "\n",
    "#convert to pandas df\n",
    "aic=pd.read_html(x.summary().as_html(),header=0, index_col=0)[0][['AIC']]\n",
    "aic['AIC']=[float(i[0:6]) for i in aic.values.flatten()]\n",
    "\n",
    "#find the first minimum in the series (not the lowest AIC but the first, lowest AIC)\n",
    "asign=np.sign(aic.diff()) #first derivative\n",
    "signchange = ((np.roll(asign, 1) - asign) != 0).astype(int) #where does the sign change?\n",
    "lag_order=np.nonzero(~signchange.astype(bool).AIC.values)[0][0] #find first sign change\n",
    "\n",
    "#plot\n",
    "aic.plot()\n",
    "plt.scatter(lag_order, aic.loc[lag_order], color='r', label='best lag order')\n",
    "plt.title('AIC Values')\n",
    "plt.ylabel('AIC')\n",
    "plt.xlabel('Lag Order')\n",
    "plt.legend()\n",
    "print(\"Lag order to use is \"+str(lag_order))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and fit a model on the data\n",
    "\n",
    "Using the lag order defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=VAR(ndvi, freq=freq)\n",
    "model_fit = model.fit(lag_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteratively loop through the window size (for n number of windows) and forecast "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_windows = int((len(ndvi) / forecast_length) - 1)\n",
    "window_size = forecast_length\n",
    "\n",
    "aa = window_size\n",
    "dfs=[]\n",
    "for i in range(0, n_windows):\n",
    "    start=aa+lag_order\n",
    "    end=(aa)\n",
    "    backtest_input = ndvi.values[-start:-end] \n",
    "    fc = model_fit.forecast(y=backtest_input, steps=window_size)  \n",
    "    if i == 0:\n",
    "        index=ndvi.index[-end:]\n",
    "    else:\n",
    "        index=ndvi.index[-end:-(end-window_size)]\n",
    "    fc = pd.DataFrame(fc, index=index, columns=ndvi.columns)\n",
    "    dfs.append(fc)\n",
    "    aa+=window_size                       \n",
    "\n",
    "#concat results together\n",
    "fc=pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check RMSE of each variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=ndvi[ndvi.index.isin(fc.index)]\n",
    "for i in test.columns:\n",
    "    print('rmse value for', i, 'is : ', round(rmse(fc[[i]], test[[i]])[0],2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the results of the back test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ndvi.columns:\n",
    "    plt.figure(figsize=(11,5))\n",
    "    plt.plot(ndvi.index, ndvi[col], label=col+' observation',linestyle='dashed', marker='o')\n",
    "    fc[col].plot(label=col+' forecast',linestyle='dashed', marker='o')\n",
    "    plt.ylabel(col)\n",
    "    plt.title(col+\": forecast vs actual\")\n",
    "    plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make final predictions\n",
    "model = VAR(endog=ndvi,freq=freq)\n",
    "model_fit = model.fit(lag_order)\n",
    "\n",
    "pred = model_fit.forecast(ndvi.values[-model_fit.k_ar:], steps=forecast_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting predictions to dataframe\n",
    "cols = ndvi.columns\n",
    "fc = pd.DataFrame(index=range(0,len(pred)), columns=[cols])\n",
    "for j in range(0,len(cols)):\n",
    "    for i in range(0, len(pred)):\n",
    "        fc.iloc[i][j] = pred[i][j]\n",
    "\n",
    "fc.index = pd.date_range(freq=freq, start=ndvi.index[-1], periods=len(fc)+1)[1:]\n",
    "fc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the result of our forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(11,5))\n",
    "plt.plot(ndvi.index, ndvi['NDVI'], label='observations',linestyle='dashed', marker='o')\n",
    "plt.plot(fc.index, fc[['NDVI']], label='forecast',linestyle='dashed', marker='o')\n",
    "plt.ylabel('NDVI')\n",
    "plt.title(\"NDVI Forecast\")\n",
    "plt.ylim(0.0,0.9)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Additional information\n",
    "\n",
    "**License:** The code in this notebook is licensed under the [Apache License, Version 2.0](https://www.apache.org/licenses/LICENSE-2.0). \n",
    "Digital Earth Australia data is licensed under the [Creative Commons by Attribution 4.0](https://creativecommons.org/licenses/by/4.0/) license.\n",
    "\n",
    "**Contact:** If you need assistance, please post a question on the [Open Data Cube Slack channel](http://slack.opendatacube.org/) or on the [GIS Stack Exchange](https://gis.stackexchange.com/questions/ask?tags=open-data-cube) using the `open-data-cube` tag (you can view previously asked questions [here](https://gis.stackexchange.com/questions/tagged/open-data-cube)).\n",
    "If you would like to report an issue with this notebook, you can file one on [Github](https://github.com/GeoscienceAustralia/dea-notebooks).\n",
    "\n",
    "**Last modified:** May 2020\n",
    "\n",
    "**Compatible datacube version:** "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
